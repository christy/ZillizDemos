{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369c3444",
   "metadata": {},
   "source": [
    "# ReadtheDocs Retrieval Augmented Generation (RAG) using Zilliz Free Tier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffd11a",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use Milvus documentation pages to create a chatbot about our product.  The chatbot is going to follow RAG steps to retrieve chunks of data using Semantic Vector Search, then the Question + Context will be fed as a Prompt to a LLM to generate an answer.\n",
    "\n",
    "Many RAG demos use OpenAI for the Embedding Model and ChatGPT for the Generative AI model.  **In this notebook, we will demo a fully open source RAG stack.**\n",
    "\n",
    "Using open-source Q&A with retrieval saves money since we make free calls to our own data almost all the time - retrieval, evaluation, and development iterations.  We only make a paid call to OpenAI once for the final chat generation step. \n",
    "\n",
    "<div>\n",
    "<img src=\"../images/rag_image.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2509fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab install these libraries in this order:\n",
    "# !python -m pip install torch transformers sentence-transformers langchain\n",
    "# !python -m pip install pymilvus==v2.3.6\n",
    "# !python -m pip install unstructured openai tqdm numpy ipykernel \n",
    "# !python -m pip install ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7570b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries.\n",
    "import sys, os, time, pprint\n",
    "import numpy as np\n",
    "\n",
    "# Import custom functions for splitting and search.\n",
    "sys.path.append(\"../\")  # Adds higher directory to python modules path.\n",
    "import milvus_utilities as _utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059b674",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "The data used in this notebook is Milvus documentation web pages.\n",
    "\n",
    "The code block below downloads all the web pages into a local directory called `rtdocs`.  \n",
    "\n",
    "I've already uploaded the `rtdocs` data folder to github, so you should see it if you cloned my repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25686cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 23 documents\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT TO DOWNLOAD THE DOCS.\n",
    "\n",
    "# !pip install -U langchain\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "DOCS_PAGE=\"https://milvus.io/docs/\"\n",
    "\n",
    "loader = RecursiveUrlLoader(DOCS_PAGE)\n",
    "docs = loader.load()\n",
    "\n",
    "num_documents = len(docs)\n",
    "print(f\"loaded {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b232dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT TO READ THE DOCS FROM A LOCAL DIRECTORY.\n",
    "\n",
    "# # Read docs into LangChain\n",
    "# # !pip install -U langchain\n",
    "# # !pip install unstructured\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# # Load HTML files from a local directory\n",
    "# path = \"../../RAG/rtdocs/\"\n",
    "# loader = DirectoryLoader(path, glob='*.html')\n",
    "# docs = loader.load()\n",
    "\n",
    "# num_documents = len(docs)\n",
    "# print(f\"loaded {num_documents} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d758e4",
   "metadata": {},
   "source": [
    "# Connect to Zilliz free tier cluster\n",
    "To use fully-managed Milvus on [Ziliz Cloud free trial](https://cloud.zilliz.com/login).  \n",
    "  1. Choose the default \"Starter\" option and accept the default Cloud Provider and Region when you create a cluster. \n",
    "  2. On the Cluster main page, copy your `API Key` and store it locally in a .env variable.  See [this note](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety) how to do that.\n",
    "  3. Also on the Cluster main page, copy the `Public Endpoint URI` and store it somewhere convenient.\n",
    "  4. Jupyter also requires them in a local .env file. <br>\n",
    "Anywhere in the bootcamp directory, create a .env file\n",
    "Insert lines like this, substituting your actual API keys for the sample text: <br>\n",
    "ZILLIZ_API_KEY=f370c <br>\n",
    "OPENAI_API_KEY=sk-H <br>\n",
    "ANYSCALE_ENPOINT_KEY=es <br>\n",
    "ANTHROPIC_API_KEY=sk-an <br>\n",
    "VARIABLE_NAME=value <br>\n",
    "Save the .env file <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0806d2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymilvus version: 2.4.2\n",
      "Type of server: Zilliz Cloud Vector Database(Compatible with Milvus 2.4)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1. CONNECT TO ZILLIZ CLOUD\n",
    "import os\n",
    "import pymilvus\n",
    "print(f\"pymilvus version: {pymilvus.__version__}\")\n",
    "from pymilvus import connections, utility, MilvusClient\n",
    "TOKEN = os.getenv(\"ZILLIZ_API_KEY\")\n",
    "\n",
    "# Connect to Zilliz cloud using endpoint URI and API key TOKEN.\n",
    "# TODO change this.\n",
    "CLUSTER_ENDPOINT=\"https://in03-xxxx.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "CLUSTER_ENDPOINT=\"https://in03-48a5b11fae525c9.api.gcp-us-west1.zillizcloud.com:443\"\n",
    "connections.connect(\n",
    "  alias='default',\n",
    "  #  Public endpoint obtained from Zilliz Cloud\n",
    "  uri=CLUSTER_ENDPOINT,\n",
    "  # API key or a colon-separated cluster username and password\n",
    "  token=TOKEN,\n",
    ")\n",
    "\n",
    "# Use no-schema Milvus client uses flexible json key:value format.\n",
    "# https://milvus.io/docs/using_milvusclient.md\n",
    "mc = MilvusClient(\n",
    "    uri=CLUSTER_ENDPOINT,\n",
    "    # API key or a colon-separated cluster username and password\n",
    "    token=TOKEN)\n",
    "\n",
    "# Check if the server is ready and get colleciton name.\n",
    "print(f\"Type of server: {utility.get_server_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103354dc",
   "metadata": {},
   "source": [
    "## Optional - Start up Milvus running in local Docker\n",
    "\n",
    ">â›”ï¸ Make sure you pip install the correct version of pymilvus and server yml file.  **Versions (major and minor) should all match**.\n",
    "\n",
    "1. [Install Docker](https://docs.docker.com/get-docker/)\n",
    "2. Start your Docker Desktop\n",
    "3. Download the latest [docker-compose.yml](https://milvus.io/docs/install_standalone-docker.md#Download-the-YAML-file) (or run the wget command, replacing version to what you are using)\n",
    "> wget https://github.com/milvus-io/milvus/releases/download/v2.4.0-rc.1/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "4. From your terminal:  \n",
    "   - cd into directory where you saved the .yml file (usualy same dir as this notebook)\n",
    "   - docker compose up -d\n",
    "   - verify (either in terminal or on Docker Desktop) the containers are running\n",
    "5. From your code (see notebook code below):\n",
    "   - Import milvus\n",
    "   - Connect to the local milvus server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STEP 1. CONNECT TO MILVUS STANDALONE DOCKER.\n",
    "\n",
    "# import pymilvus, time\n",
    "# from pymilvus import (\n",
    "#     MilvusClient, utility, connections,\n",
    "#     FieldSchema, CollectionSchema, DataType, IndexType,\n",
    "#     Collection, AnnSearchRequest, RRFRanker, WeightedRanker\n",
    "# )\n",
    "# print(f\"Pymilvus: {pymilvus.__version__}\")\n",
    "\n",
    "# # Connect to the local server.\n",
    "# connection = connections.connect(\n",
    "#   alias=\"default\", \n",
    "#   host='localhost', # or '0.0.0.0' or 'localhost'\n",
    "#   port='19530'\n",
    "# )\n",
    "\n",
    "# # Get server version.\n",
    "# print(utility.get_server_version())\n",
    "\n",
    "# # Use no-schema Milvus client uses flexible json key:value format.\n",
    "# mc = MilvusClient(connections=connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39af3fd",
   "metadata": {},
   "source": [
    "## Load the Embedding Model checkpoint and use it to create vector embeddings\n",
    "\n",
    "#### What are Embeddings?\n",
    "\n",
    "Check out [this blog](https://zilliz.com/glossary/vector-embeddings) for an introduction to embeddings.  \n",
    "\n",
    "An excellent place to start is by selecting an embedding model from the [HuggingFace MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), sorted descending by the \"Retrieval Average'' column since this task is most relevant to RAG. Then, choose the smallest, highest-ranking embedding model. But, Beware!! some models listed are overfit to the training data, so they won't perform on your data as promised.  \n",
    "\n",
    "Milvus (and Zilliz) only supports tested embedding models that are **not overfit**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -U sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1805f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py311-ray/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: BAAI/bge-m3\n",
      "EMBEDDING_DIM: 1024\n",
      "MAX_SEQ_LENGTH: 8192\n"
     ]
    }
   ],
   "source": [
    "# STEP 2. DOWNLOAD AN OPEN SOURCE EMBEDDING MODEL.\n",
    "\n",
    "# Import torch.\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize torch settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model from huggingface model hub.\n",
    "# model_name = \"WhereIsAI/UAE-Large-V1\"\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "# print(encoder)\n",
    "\n",
    "# Get the model parameters and save for later.\n",
    "EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\n",
    "MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length() \n",
    "# # Assume tokens are 3 characters long.\n",
    "# MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS * 3\n",
    "# HF_EOS_TOKEN_LENGTH = 1 * 3\n",
    "# Test with 512 sequence length.\n",
    "MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS\n",
    "HF_EOS_TOKEN_LENGTH = 1\n",
    "\n",
    "# Inspect model parameters.\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "print(f\"MAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609497f",
   "metadata": {},
   "source": [
    "## Create a Milvus collection\n",
    "\n",
    "You can think of a collection in Milvus like a \"table\" in SQL databases.  The **collection** will contain the \n",
    "- **Schema** (or [no-schema Milvus client](https://milvus.io/docs/using_milvusclient.md)).  \n",
    "ðŸ’¡ You'll need the vector `EMBEDDING_DIM` parameter from your embedding model.\n",
    "Typical values are:\n",
    "   - 1024 for sbert embedding models\n",
    "   - 1536 for ada-002 OpenAI embedding models\n",
    "- **Vector index** for efficient vector search\n",
    "- **Vector distance metric** for measuring nearest neighbor vectors\n",
    "- **Consistency level**\n",
    "In Milvus, transactional consistency is possible; however, according to the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem), some latency must be sacrificed. ðŸ’¡ Searching movie reviews is not mission-critical, so [`eventually`](https://milvus.io/docs/consistency.md) consistent is fine here.\n",
    "\n",
    "## Add a Vector Index\n",
    "\n",
    "The vector index determines the vector **search algorithm** used to find the closest vectors in your data to the query a user submits.  \n",
    "\n",
    "Most vector indexes use different sets of parameters depending on whether the database is:\n",
    "- **inserting vectors** (creation mode) - vs - \n",
    "- **searching vectors** (search mode) \n",
    "\n",
    "Scroll down the [docs page](https://milvus.io/docs/index.md) to see a table listing different vector indexes available on Milvus.  For example:\n",
    "- FLAT - deterministic exhaustive search\n",
    "- IVF_FLAT or IVF_SQ8 - Hash index (stochastic approximate search)\n",
    "- HNSW - Graph index (stochastic approximate search)\n",
    "- AUTOINDEX - OSS or [Zilliz cloud](https://docs.zilliz.com/docs/autoindex-explained) automatic index based on type of GPU, size of data.\n",
    "\n",
    "Besides a search algorithm, we also need to specify a **distance metric**, that is, a definition of what is considered \"close\" in vector space.  In the cell below, the [`HNSW`](https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md) search index is chosen.  Its possible distance metrics are one of:\n",
    "- L2 - L2-norm\n",
    "- IP - Dot-product\n",
    "- COSINE - Angular distance\n",
    "\n",
    "ðŸ’¡ Most use cases work better with normalized embeddings, in which case L2 is useless (every vector has length=1) and IP and COSINE are the same.  Only choose L2 if you plan to keep your embeddings unnormalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dff3b3",
   "metadata": {},
   "source": [
    "### Exercise #1 (2 min):\n",
    "Create a collection named \"movies\".  Use the default AUTOINDEX.\n",
    "> ðŸ’¡ AUTOINDEX works on both Milvus and Zilliz Cloud (where it is the fastest!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6197605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymilvus import MilvusClient\n",
    "\n",
    "# # Set the Milvus collection name.\n",
    "# COLLECTION_NAME = #(exercise): code here\n",
    "# # hint:  COLLECTION_NAME = \"MilvusDocs\"\n",
    "\n",
    "# # Use no-schema Milvus client uses flexible json key:value format.\n",
    "# # https://milvus.io/docs/using_milvusclient.md\n",
    "# mc = MilvusClient(\n",
    "#     uri=CLUSTER_ENDPOINT,\n",
    "#     # API key or a colon-separated cluster username and password\n",
    "#     token=TOKEN)\n",
    "\n",
    "# mc.drop_collection(COLLECTION_NAME)\n",
    "# mc.create_collection(COLLECTION_NAME, \n",
    "#                      EMBEDDING_DIM, \n",
    "#                     )\n",
    "\n",
    "# print(mc.describe_collection(COLLECTION_NAME))\n",
    "# print(f\"Created collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605bd48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created collection: `MilvusDocs`\n"
     ]
    }
   ],
   "source": [
    "# STEP 3. CREATE A NO-SCHEMA MILVUS COLLECTION AND DEFINE THE DATABASE INDEX.\n",
    "\n",
    "# Set the Milvus collection name.\n",
    "COLLECTION_NAME = \"MilvusDocs\"\n",
    "\n",
    "# Add custom HNSW search index to the collection.\n",
    "# M = max number graph connections per layer. Large M = denser graph.\n",
    "# Choice of M: 4~64, larger M for larger data and larger embedding lengths.\n",
    "M = 16\n",
    "# efConstruction = num_candidate_nearest_neighbors per layer. \n",
    "# Use Rule of thumb: int. 8~512, efConstruction = M * 2.\n",
    "efConstruction = M * 2\n",
    "# Create the search index for local Milvus server.\n",
    "INDEX_PARAMS = dict({\n",
    "    'M': M,               \n",
    "    \"efConstruction\": efConstruction })\n",
    "index_params = {\n",
    "    \"index_type\": \"HNSW\", \n",
    "    \"metric_type\": \"COSINE\", \n",
    "    \"params\": INDEX_PARAMS\n",
    "    }\n",
    "\n",
    "# Use no-schema Milvus client uses flexible json key:value format.\n",
    "# https://milvus.io/docs/using_milvusclient.md\n",
    "mc = MilvusClient(\n",
    "    uri=CLUSTER_ENDPOINT,\n",
    "    # API key or a colon-separated cluster username and password\n",
    "    token=TOKEN)\n",
    "\n",
    "# Check if collection already exists, if so drop it.\n",
    "has = utility.has_collection(COLLECTION_NAME)\n",
    "if has:\n",
    "    drop_result = utility.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")\n",
    "\n",
    "# Create the collection.\n",
    "mc.create_collection(COLLECTION_NAME, \n",
    "                     EMBEDDING_DIM,\n",
    "                     consistency_level=\"Eventually\", \n",
    "                     auto_id=True,  \n",
    "                     overwrite=True,\n",
    "                     # skip setting params below, if using AUTOINDEX\n",
    "                     params=index_params\n",
    "                    )\n",
    "\n",
    "print(f\"Successfully created collection: `{COLLECTION_NAME}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2cac7c",
   "metadata": {},
   "source": [
    "## Simple Chunking\n",
    "\n",
    "Before embedding, it is necessary to decide your chunk strategy, chunk size, and chunk overlap.  This section uses:\n",
    "- **Strategy** = Simple fixed chunk lengths.\n",
    "- **Chunk size** = Use the embedding model's parameter `MAX_SEQ_LENGTH`\n",
    "- **Overlap** = Rule-of-thumb 10-15%\n",
    "- **Function** = \n",
    "  - Langchain's `RecursiveCharacterTextSplitter` to split up long reviews recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c00e6e",
   "metadata": {},
   "source": [
    "### Exercise #2 (2 min):\n",
    "Change the chunk_size and see what happens?  Model default is 512.\n",
    "\n",
    "- What do your observations imply about changing the chunk_size and the number of vectors?\n",
    "- How many vectors are there with chunk_size=256?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1499b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import numpy as np\n",
    "# import pprint\n",
    "\n",
    "# ###############\n",
    "# ## EXERCISE #2: Change chunk_size to 256 below.  How many chunks (vectors) does this create?\n",
    "# ## ANSWER:  804\n",
    "# ## BONUS:   Can you explain why the number of vectors changed from 362 to 804?  \n",
    "# ##          Hint:  What is the default chunk overlap?  362 * (2 + 0.10) approx. equals 804.\n",
    "# ###############\n",
    "# chunk_size = #(exercise): code here\n",
    "# chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "# print(f\"chunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}\")\n",
    "\n",
    "# # Create an instance of the RecursiveCharacterTextSplitter\n",
    "# child_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size = chunk_size,\n",
    "#     chunk_overlap = chunk_overlap,\n",
    "#     length_function = len,  # using built-in Python len function\n",
    "# )\n",
    "\n",
    "# # Split the documents further into smaller, recursive chunks.\n",
    "# chunks = child_splitter.split_documents(docs)\n",
    "# print(f\"docs: {len(docs)}, split into: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075a3022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 512, chunk_overlap: 51.0\n",
      "chunking time: 6.954904079437256\n",
      "docs: 23, split into: 36266\n",
      "split into chunks: 36266, type: list of <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Looking at a sample chunk...\n",
      "<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><meta http-equiv=\"x-ua-compatible\" conte\n",
      "{'source': 'https://milvus.io/docs/', 'title': 'Milvus documentation', 'description': 'Milvus documentation for Milvus v2.4.x', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install lxml\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "# Define chunk size 512 and overlap 10% chunk_size.\n",
    "chunk_size = 512\n",
    "chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "print(f\"chunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}\")\n",
    "\n",
    "# Create an instance of the RecursiveCharacterTextSplitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    "    length_function = len,  # using built-in Python len function\n",
    ")\n",
    "\n",
    "# Split the documents further into smaller, recursive chunks.\n",
    "start_time = time.time()\n",
    "chunks = child_splitter.split_documents(docs)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"chunking time: {end_time - start_time}\")\n",
    "print(f\"docs: {len(docs)}, split into: {len(chunks)}\")\n",
    "print(f\"split into chunks: {len(chunks)}, type: list of {type(chunks[0])}\") \n",
    "\n",
    "# Clean up newlines in the chunks.\n",
    "for chunk in chunks:\n",
    "    chunk.page_content = chunk.page_content.replace(\"\\n\", \" \")\n",
    "    \n",
    "# Clean up the metadata urls\n",
    "for chunk in chunks:\n",
    "    new_url = chunk.metadata[\"source\"]\n",
    "    new_url = new_url.replace(\"../RAG/rtdocs\", \"https://milvus.io/docs\")\n",
    "    new_url = new_url.replace(\".html\", \".md\")\n",
    "    chunk.metadata.update({\"source\": new_url})\n",
    "\n",
    "# Inspect a chunk.\n",
    "print()\n",
    "print(\"Looking at a sample chunk...\")\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60423a5",
   "metadata": {},
   "source": [
    "## HTML Chunking\n",
    "\n",
    "Before embedding, it is necessary to decide your chunk strategy, chunk size, and chunk overlap.  This section uses:\n",
    "- **Strategy** = Use markdown header hierarchies.  Keep markdown sections together unless they are too long.\n",
    "- **Chunk size** = Use the embedding model's parameter `MAX_SEQ_LENGTH`\n",
    "- **Overlap** = Rule-of-thumb 10-15%\n",
    "- **Function** = \n",
    "  - Langchain's `HTMLHeaderTextSplitter` to split markdown sections.\n",
    "  - Langchain's `RecursiveCharacterTextSplitter` to split up long reviews recursively.\n",
    "\n",
    "\n",
    "Notice below, each chunk is grounded with the document source page.  <br>\n",
    "In addition, header titles are kept together with the chunk of markdown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab9cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking time: 0.435960054397583\n",
      "docs: 23, split into: 222\n",
      "split into chunks: 569, type: list of <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Looking at a sample chunk...\n",
      "Why MilvusDocsTutorialsToolsBlogCommunity  \n",
      "Try Managed Milvus FREE  \n",
      "Stars0  \n",
      "Search  \n",
      "Home  \n",
      "Ã¢Â€Â‹  \n",
      "{'h1': 'Why MilvusDocsTutorialsToolsBlogCommunity  \\nTry Managed Milvus FREE  \\nStars0  \\nSearch  \\nHome  \\nÃ¢\\x80\\x8b  ', 'source': 'https://milvus.io/docs/'}\n",
      "Why MilvusDocsTutorialsToolsBlogCommunity  \n",
      "Try Managed Milvus FREE  \n",
      "Stars0  \n",
      "Search  \n",
      "Home  \n",
      "Ã¢Â€Â‹  \n",
      "{'h1': 'Why MilvusDocsTutorialsToolsBlogCommunity  \\nTry Managed Milvus FREE  \\nStars0  \\nSearch  \\nHome  \\nÃ¢\\x80\\x8b  ', 'source': 'https://milvus.io/docs/'}\n"
     ]
    }
   ],
   "source": [
    "# # STEP 4. PREPARE DATA: CHUNK AND EMBED\n",
    "\n",
    "# Define the headers to split on for the HTMLHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "]\n",
    "# Create an instance of the HTMLHeaderTextSplitter\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Split the HTML text using the HTMLHeaderTextSplitter.\n",
    "start_time = time.time()\n",
    "html_header_splits = []\n",
    "for doc in docs:\n",
    "    splits = html_splitter.split_text(doc.page_content)\n",
    "    for split in splits:\n",
    "        # Add the source URL and header values to the metadata\n",
    "        metadata = {}\n",
    "        new_text = split.page_content\n",
    "        for header_name, metadata_header_name in headers_to_split_on:\n",
    "            # Handle exception if h1 does not exist.\n",
    "            try:\n",
    "                header_value = new_text.split(\"Â¶ \")[0].strip()[:100]\n",
    "                metadata[header_name] = header_value\n",
    "            except:\n",
    "                break\n",
    "            # Handle exception if h2 does not exist.\n",
    "            try:\n",
    "                new_text = new_text.split(\"Â¶ \")[1].strip()[:50]\n",
    "            except:\n",
    "                break\n",
    "        split.metadata = {\n",
    "            **metadata,\n",
    "            \"source\": doc.metadata[\"source\"]\n",
    "        }\n",
    "        # Add the header to the text\n",
    "        split.page_content = split.page_content\n",
    "    html_header_splits.extend(splits)\n",
    "\n",
    "    # # TODO - Uncomment to save each doc.page_content as a local html file under OUTPUT_DIR.\n",
    "    # OUTPUT_DIR = \"output\"\n",
    "    # # Set filename to first 50 characters of h1 header.\n",
    "    # filename = doc.metadata[\"source\"].split(\"/\")[-1].split(\".\")[0][:50]\n",
    "    # with open(f\"{OUTPUT_DIR}/{filename}.html\", \"w\") as f:\n",
    "    #     f.write(doc.page_content)\n",
    "\n",
    "# Split the documents further into smaller, recursive chunks.\n",
    "chunks = child_splitter.split_documents(html_header_splits)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"chunking time: {end_time - start_time}\")\n",
    "print(f\"docs: {len(docs)}, split into: {len(html_header_splits)}\")\n",
    "print(f\"split into chunks: {len(chunks)}, type: list of {type(chunks[0])}\") \n",
    "\n",
    "# Inspect a chunk.\n",
    "print()\n",
    "print(\"Looking at a sample chunk...\")\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)\n",
    "\n",
    "# # TODO - Uncomment to print child splits with their associated header metadata for debugging.\n",
    "# print()\n",
    "# for child in chunks:\n",
    "#     print(f\"Content: {child.page_content}\")\n",
    "#     print(f\"Metadata: {child.metadata}\")\n",
    "#     print()\n",
    "\n",
    "# Clean up the metadata urls\n",
    "for doc in chunks:\n",
    "    new_url = doc.metadata[\"source\"]\n",
    "    new_url = new_url.replace(\"../../RAG/rtdocs\", \"https://milvus.io/docs\")\n",
    "    new_url = new_url.replace(\".html\", \".md\")\n",
    "    doc.metadata.update({\"source\": new_url})\n",
    "\n",
    "print(chunks[0].page_content[:100])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d223c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time for 569 chunks: 67.81 seconds\n"
     ]
    }
   ],
   "source": [
    "# STEP 5. TRANSFORM CHUNKS INTO VECTORS USING EMBEDDING MODEL INFERENCE.\n",
    "\n",
    "# Encoder input is docs as a list of strings.\n",
    "list_of_strings = [doc.page_content for doc in chunks if hasattr(doc, 'page_content')]\n",
    "\n",
    "# Embedding inference using the Milvus built-in sparse-dense-reranking encoder.\n",
    "start_time = time.time()\n",
    "embeddings = torch.tensor(encoder.encode(list_of_strings))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Embedding time for {len(list_of_strings)} chunks: \", end=\"\")\n",
    "print(f\"{np.round(end_time - start_time, 2)} seconds\")\n",
    "\n",
    "# Inference Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:35<00:00,  1.86s/it]\n",
    "# Embedding time for 304 chunks: 35.74 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babb0388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type embeddings: <class 'list'> of <class 'numpy.ndarray'>\n",
      "of numbers: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# Normalize the embeddings.\n",
    "embeddings = np.array(embeddings / np.linalg.norm(embeddings))\n",
    "\n",
    "# Convert embeddings to list of `numpy.ndarray`, each containing `numpy.float32` numbers.\n",
    "converted_values = list(map(np.float32, embeddings))\n",
    "\n",
    "# Inspect the embeddings.\n",
    "assert len(chunks[0].page_content) <= MAX_SEQ_LENGTH-1\n",
    "assert len(converted_values[0]) == EMBEDDING_DIM\n",
    "print(f\"type embeddings: {type(converted_values)} of {type(converted_values[0])}\")\n",
    "print(f\"of numbers: {type(converted_values[0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd8153",
   "metadata": {},
   "source": [
    "## Insert data into Milvus\n",
    "\n",
    "For each original text chunk, we'll write the sextuplet (`chunk, h1, h2, source, dense_vector, sparse_vector`) into the database.\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/db_insert.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "**The Milvus Client wrapper can only handle loading data from a list of dictionaries.**\n",
    "\n",
    "Otherwise, in general, Milvus supports loading data from:\n",
    "- pandas dataframes \n",
    "- list of dictionaries\n",
    "\n",
    "Below, we use the embedding model provided by HuggingFace, download its checkpoint, and run it locally as the encoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79dd2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n",
      "<class 'dict'> 5\n",
      "{'chunk': 'Why MilvusDocsTutorialsToolsBlogCommunity  \\n'\n",
      "          'Try Managed Milvus FREE  \\n'\n",
      "          'Stars0  \\n'\n",
      "          'Search  \\n'\n",
      "          'Home  \\n'\n",
      "          'Ã¢\\x80\\x8b  \\n'\n",
      "          'v2.4.x  \\n'\n",
      "          'About Milvus  \\n'\n",
      "          'Get Started  \\n'\n",
      "          'Concepts  \\n'\n",
      "          'User Guide  \\n'\n",
      "          'Models  \\n'\n",
      "          'Administration Guide  \\n'\n",
      "          'Tools  \\n'\n",
      "          'Integrations  \\n'\n",
      "          'Example Applications  \\n'\n",
      "          'FAQs  \\n'\n",
      "          'API reference  \\n'\n",
      "          'Blog  \\n'\n",
      "          'Get Started Recommended articles  \\n'\n",
      "          'Welcome to Milvus Docs! Here you will learn about what Milvus is, '\n",
      "          'and how to install, use, and deploy Milvus to build an application '\n",
      "          'according to your business need.  \\n'\n",
      "          'Try Managed Milvus For Free!',\n",
      " 'h1': 'Why MilvusDocsTutorialsToolsBlogCommunity  \\nTry Ma',\n",
      " 'h2': '',\n",
      " 'source': 'https://milvus.io/docs/',\n",
      " 'vector': array([-0.0015468 ,  0.00018235, -0.00156189, ..., -0.00015907,\n",
      "        0.00154837, -0.0012311 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# STEP 6. INSERT CHUNK LIST INTO MILVUS OR ZILLIZ.\n",
    "\n",
    "# Create chunk_list and dict_list in a single loop\n",
    "dict_list = []\n",
    "for chunk, vector in zip(chunks, converted_values):\n",
    "    # Assemble embedding vector, original text chunk, metadata.\n",
    "    chunk_dict = {\n",
    "        'chunk': chunk.page_content,\n",
    "        'h1': chunk.metadata.get('h1', \"\")[:50],\n",
    "        'h2': chunk.metadata.get('h2', \"\")[:50],\n",
    "        'source': chunk.metadata.get('source', \"\"),\n",
    "        'vector': vector,\n",
    "    }\n",
    "    dict_list.append(chunk_dict)\n",
    "\n",
    "# TODO - Uncomment to inspect the first chunk and its metadata.\n",
    "print(len(dict_list))\n",
    "print(type(dict_list[0]), len(dict_list[0]))\n",
    "pprint.pprint(dict_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ac0d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting entities\n",
      "Milvus insert time for 569 vectors: 1.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Insert data into the Milvus collection.\n",
    "print(\"Start inserting entities\")\n",
    "start_time = time.time()\n",
    "# mc.insert(dict_list)\n",
    "mc.insert(\n",
    "    COLLECTION_NAME,\n",
    "    data=dict_list,\n",
    "    progress_bar=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Milvus insert time for {len(dict_list)} vectors: \", end=\"\")\n",
    "print(f\"{np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c589ff",
   "metadata": {},
   "source": [
    "## Ask a question about your data\n",
    "\n",
    "So far in this demo notebook: \n",
    "1. Your custom data has been mapped into a vector embedding space\n",
    "2. Those vector embeddings have been saved into a vector database\n",
    "\n",
    "Next, you can ask a question about your custom data!\n",
    "\n",
    "ðŸ’¡ In LLM vocabulary:\n",
    "> **Query** is the generic term for user questions.  \n",
    "A query is a list of multiple individual questions, up to maybe 1000 different questions!\n",
    "\n",
    "> **Question** usually refers to a single user question.  \n",
    "In our example below, the user question is \"What is AUTOINDEX in Milvus Client?\"\n",
    "\n",
    "> **Semantic Search** = very fast search of the entire knowledge base to find the `TOP_K` documentation chunks with the closest embeddings to the user's query.\n",
    "\n",
    "ðŸ’¡ The same model should always be used for consistency for all the embeddings data and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e7f41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query length: 75\n"
     ]
    }
   ],
   "source": [
    "# Define a sample question about your data.\n",
    "QUESTION1 = \"What do the parameters for HNSW mean?\"\n",
    "QUESTION2 = \"What are good default values for HNSW parameters with 25K vectors dim 1024?\"\n",
    "QUESTION3 = \"What does nlist vs nprobe mean in ivf_flat?\"\n",
    "QUESTION4 = \"What is the default AUTOINDEX index and vector field distance metric in Milvus?\"\n",
    "\n",
    "# In case you want to ask all the questions at once.\n",
    "QUERY = [QUESTION1, QUESTION2, QUESTION3, QUESTION4]\n",
    "\n",
    "# Inspect the length of one question.\n",
    "QUERY_LENGTH = len(QUESTION2)\n",
    "print(f\"query length: {QUERY_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd25ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT A PARTICULAR QUESTION TO ASK.\n",
    "\n",
    "SAMPLE_QUESTION = QUESTION1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea29411",
   "metadata": {},
   "source": [
    "## Execute a vector search\n",
    "\n",
    "Search Milvus using [PyMilvus API](https://milvus.io/docs/search.md).\n",
    "\n",
    "ðŸ’¡ By their nature, vector searches are \"semantic\" searches.  For example, if you were to search for \"leaky faucet\": \n",
    "> **Traditional Key-word Search** - either or both words \"leaky\", \"faucet\" would have to match some text in order to return a web page or link text to the document.\n",
    "\n",
    "> **Semantic search** - results containing words \"drippy\" \"taps\" would be returned as well because these words mean the same thing even though they are different words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c2758",
   "metadata": {},
   "source": [
    "### Exercise #3 (2 min):\n",
    "Search Milvus using the default search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "820dc05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run semantic vector search using your query and the vector database.\n",
    "\n",
    "# # Embed the question using the same encoder.\n",
    "# query_embeddings = _utils.embed_query(encoder, [SAMPLE_QUESTION])\n",
    "\n",
    "# # Uses default search algorithm:  HNSW and top_k=10.\n",
    "# start_time = time.time()\n",
    "# results = mc.search(\n",
    "#     COLLECTION_NAME,\n",
    "#     data=query_embeddings, \n",
    "#     )\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"Search time: {elapsed_time} sec\")\n",
    "\n",
    "# # Inspect search result.\n",
    "# print(f\"type: {type(results)}, count: {len(results[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f0cc47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output fields: ['chunk', 'h1', 'h2', 'source']\n"
     ]
    }
   ],
   "source": [
    "# Define metadata fields you can filter on.\n",
    "OUTPUT_FIELDS = list(dict_list[0].keys())\n",
    "OUTPUT_FIELDS.remove('vector')\n",
    "print(f\"output fields: {OUTPUT_FIELDS}\")\n",
    "\n",
    "# Define a convenience function for searching.\n",
    "def mc_run_search(question, filter_expression):\n",
    "    # Embed the question using the same encoder.\n",
    "    query_embeddings = _utils.embed_query(encoder, [question])\n",
    "    TOP_K = 2\n",
    "\n",
    "    # Return top k results with HNSW index.\n",
    "    SEARCH_PARAMS = dict({\n",
    "        # Re-use index param for num_candidate_nearest_neighbors.\n",
    "        \"ef\": INDEX_PARAMS['efConstruction']\n",
    "    })\n",
    "\n",
    "    # Run semantic vector search using your query and the vector database.\n",
    "    results = mc.search(\n",
    "        COLLECTION_NAME,\n",
    "        data=query_embeddings, \n",
    "        search_params=SEARCH_PARAMS,\n",
    "        output_fields=OUTPUT_FIELDS, \n",
    "        # Milvus can utilize metadata in boolean expressions to filter search.\n",
    "        filter=filter_expression,\n",
    "        limit=TOP_K,\n",
    "        consistency_level=\"Eventually\"\n",
    "    )\n",
    "\n",
    "    # Assemble retrieved context and context metadata.\n",
    "    # The search result is in the variable `results[0]`, which is type \n",
    "    # 'pymilvus.orm.search.SearchResult'. \n",
    "    METADATA_FIELDS = [f for f in OUTPUT_FIELDS if f != 'chunk']\n",
    "    formatted_results, context, context_metadata = _utils.client_assemble_retrieved_context(\n",
    "        results, metadata_fields=METADATA_FIELDS, num_shot_answers=TOP_K)\n",
    "    \n",
    "    return formatted_results, context, context_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40734f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter: \n",
      "Milvus Client search time for 569 vectors: 0.20366311073303223 seconds\n",
      "type: <class 'list'>, count: 0\n"
     ]
    }
   ],
   "source": [
    "# STEP 7. RETRIEVE ANSWERS FROM YOUR DOCUMENTS STORED IN MILVUS OR ZILLIZ.\n",
    "\n",
    "# Metadata filters for CSV dataset.\n",
    "# expression = 'film_year >= 2019'\n",
    "expression = \"\"\n",
    "print(f\"filter: {expression}\")\n",
    "\n",
    "start_time = time.time()\n",
    "formatted_results, contexts, context_metadata = \\\n",
    "    mc_run_search(SAMPLE_QUESTION, expression)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Milvus Client search time for {len(dict_list)} vectors: {elapsed_time} seconds\")\n",
    "\n",
    "# Inspect search result.\n",
    "print(f\"type: {type(formatted_results)}, count: {len(formatted_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cade1",
   "metadata": {},
   "source": [
    "## Assemble and inspect the search result\n",
    "\n",
    "The search result is in the variable `results[0]` consisting of top_k-count of objects of type `'pymilvus.client.abstract.Hits'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7a2a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through search results, print metadata.\n",
    "sources = []\n",
    "for i in range(len(contexts)):\n",
    "    print(f\"Retrieved result #{i+1}\")\n",
    "    print(f\"distance = {formatted_results[i][0]}\")\n",
    "    pprint.pprint(f\"Chunk text: {contexts[i]}\")\n",
    "    for key, value in context_metadata[i].items():\n",
    "        if key == \"source\":\n",
    "            sources.append(value)\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6060ce",
   "metadata": {},
   "source": [
    "## Use an LLM to Generate a chat response to the user's question using the Retrieved Context.\n",
    "\n",
    "Many different generative LLMs exist these days.  Check out the lmsys [leaderboard](https://chat.lmsys.org/?leaderboard).\n",
    "\n",
    "In this notebook, we'll try these LLMs:\n",
    "- The newly released open-source Llama 3 from Meta.\n",
    "- The cheapest, paid model from Anthropic Claude3 Haiku.\n",
    "- The standard in its price cateogory, gpt-3.5-turbo, from Openai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb4c323f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length long text to summarize: 0\n"
     ]
    }
   ],
   "source": [
    "# STEP 8. LLM-GENERATED ANSWER TO THE QUESTION, GROUNDED BY RETRIEVED CONTEXT.\n",
    "\n",
    "# Separate all the context together by space.\n",
    "# Lance Martin, LangChain, says put best contexts at end.\n",
    "contexts_combined = ' '.join(reversed(contexts))\n",
    "# Separate all the sources together by comma.\n",
    "source_combined = ' '.join(reversed(sources))\n",
    "print(f\"Length long text to summarize: {len(contexts_combined)}\")\n",
    "\n",
    "# Define temperature for the LLM and random seed.\n",
    "TEMPERATURE = 0.1\n",
    "TOP_P = 0.9\n",
    "RANDOM_SEED = 415\n",
    "MAX_TOKENS = 512\n",
    "FREQUENCY_PENALTY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc90b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"First, check if the Context below is relevant to \n",
    "the user's question.  Second if the context is not strongly relevant, \n",
    "throw away the context.  Third, if the context is strongly relevant, \n",
    "answer the question using the context; otherwise ignore the context \n",
    "and answer the question.  Be complete, clear, concise.  \n",
    "Answer with fewer than 3 sentences and cite unique sources at the end.\n",
    "Answer: The answer to the question.\n",
    "Grounding sources: {source_combined}\n",
    "Context: {contexts_combined}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb35aa",
   "metadata": {},
   "source": [
    "# Use Meta Llama 3 with Ollama to generate a human-like chat response to the user's question\n",
    "\n",
    "Follow the instructions to install ollama and pull a model.<br>\n",
    "https://github.com/ollama/ollama\n",
    "\n",
    "View details about which models are supported by ollama. <br>\n",
    "https://ollama.com/library/llama3\n",
    "\n",
    "That page says `ollama run llama3` will by default pull the latest \"instruct\" model, which is fine-tuned for chat/dialogue use cases.\n",
    "\n",
    "The other kind of llama3 models are \"pre-trained\" base model. <br>\n",
    "Example: ollama run llama3:text ollama run llama3:70b-text\n",
    "\n",
    "**Format** `gguf` means the model runs on CPU.  gg = \"Georgi Gerganov\", creator of the C library model format ggml, which was recently changed to gguf.\n",
    "\n",
    "**Quantization** (think of it like vector compaction) can lead to higher throughput at the expense of lower accuracy.  For the curious, quantization meanings can be found on: <br>\n",
    "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/tree/main.  \n",
    "\n",
    "Below just listing the main quantization types.\n",
    "- **q4_0**: Original quant method, 4-bit.\n",
    "- **q4_k_m**: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\n",
    "- **q5_0**: Higher accuracy, higher resource usage and slower inference.\n",
    "- **q5_k_m**: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\n",
    "- **q 6_k**: Uses Q8_K for all tensors\n",
    "- **q8_0**: Almost indistinguishable from float16. High resource use and slow. Not recommended for most users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0edc67e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL:llama3:latest, FORMAT:gguf, PARAMETER_SIZE:8B, QUANTIZATION_LEVEL:Q4_0, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install ollama\n",
    "import ollama\n",
    "\n",
    "# Verify details which model you are running.\n",
    "ollama_llama3 = ollama.list()['models'][0]\n",
    "\n",
    "# Print the model details.\n",
    "keys = ['format', 'parameter_size', 'quantization_level']\n",
    "print(f\"MODEL:{ollama.list()['models'][0]['name']}\", end=\", \")\n",
    "for key in keys:\n",
    "    print(f\"{str.upper(key)}:{ollama.list()['models'][0]['details'].get(key, 'Key not found in dictionary')}\", end=\", \")\n",
    "print(end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76042c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In Hierarchical Navigable Small World (HNSW) indexing, there are several key '\n",
      " 'parameters that control its performance and behavior.  * `M`: The number of '\n",
      " 'clusters in the hierarchical structure. A higher value can lead to better '\n",
      " 'recall but slower search times. * `efConstruction` and `efSearch`: These two '\n",
      " 'parameters control the trade-off between construction time (building the '\n",
      " 'index) and query time (searching for nearest neighbors). Lower values result '\n",
      " 'in faster construction, while lower values of efSearch result in faster '\n",
      " 'queries.  These are some of the most important parameters to consider when '\n",
      " 'using HNSW.')\n",
      "ollama_llama3_time: 14.31 seconds\n"
     ]
    }
   ],
   "source": [
    "# Send the question to llama 3 chat.\n",
    "start_time = time.time()\n",
    "response = ollama.chat(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {SAMPLE_QUESTION}\",}\n",
    "    ],\n",
    "    model='llama3',\n",
    "    stream=False,\n",
    "    options={\"temperature\": TEMPERATURE, \"seed\": RANDOM_SEED,\n",
    "             \"top_p\": TOP_P, \n",
    "            #  \"max_tokens\": MAX_TOKENS,\n",
    "             \"frequency_penalty\": FREQUENCY_PENALTY}\n",
    ")\n",
    "\n",
    "ollama_llama3_time = time.time() - start_time\n",
    "pprint.pprint(response['message']['content'].replace('\\n', ' '))\n",
    "print(f\"ollama_llama3_time: {format(ollama_llama3_time, '.2f')} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12b360",
   "metadata": {},
   "source": [
    "# Now try Anyscale endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59f95f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
      "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
      "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
      "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
      "OpenAI Chat: gpt-4-1106-preview\n",
      "OpenAI Chat: gpt-4-0125-preview\n",
      "OpenAI Chat: gpt-4-turbo-preview (aliases: gpt-4-turbo, 4-turbo, 4t)\n",
      "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
      "AnyscaleEndpoints: meta-llama/Llama-2-7b-chat-hf\n",
      "AnyscaleEndpoints: meta-llama/Llama-2-13b-chat-hf\n",
      "AnyscaleEndpoints: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "AnyscaleEndpoints: mistralai/Mistral-7B-Instruct-v0.1\n",
      "AnyscaleEndpoints: meta-llama/Llama-2-70b-chat-hf\n",
      "AnyscaleEndpoints: meta-llama/Llama-3-8b-chat-hf\n",
      "AnyscaleEndpoints: meta-llama/Llama-3-70b-chat-hf\n",
      "AnyscaleEndpoints: codellama/CodeLlama-70b-Instruct-hf\n",
      "AnyscaleEndpoints: mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "AnyscaleEndpoints: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
      "AnyscaleEndpoints: google/gemma-7b-it\n"
     ]
    }
   ],
   "source": [
    "# List all the anyscale endpoint models.\n",
    "!llm models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ef7529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In the Hierarchical Navigable Small World (HNSW) algorithm, the parameters '\n",
      " 'are:  * M: The number of hash tables used for indexing. * ef: The number of '\n",
      " 'nearest neighbors to search for. * efConstruction: The number of nearest '\n",
      " 'neighbors to search for during the construction of the index.  These '\n",
      " 'parameters control the trade-off between search accuracy and efficiency. '\n",
      " 'Increasing M and efConstruction can improve search accuracy, but at the cost '\n",
      " 'of increased memory usage and construction time. Increasing ef can improve '\n",
      " 'search accuracy, but at the cost of increased search time.  Source: [1] '\n",
      " '\"Hierarchical Navigable Small World\" by Miklos Sarlos and Alexander Szalay '\n",
      " '(2010)  Note: The context is strongly relevant to the question, and the '\n",
      " 'answer is based on the provided context.')\n",
      "llama3_anyscale_endpoints_time: 5.28 seconds\n"
     ]
    }
   ],
   "source": [
    "# Call Anyscale enpoint using OpenAI API.\n",
    "import openai\n",
    "\n",
    "LLM_NAME = \"meta-llama/Llama-3-8b-chat-hf\"\n",
    "\n",
    "# 2. Get your API key: https://platform.openai.com/api-keys\n",
    "# 3. Save your api key in env variable.\n",
    "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "anyscale_client = openai.OpenAI(\n",
    "    base_url = \"https://api.endpoints.anyscale.com/v1\",\n",
    "    api_key=os.environ.get(\"ANYSCALE_ENPOINT_KEY\"),\n",
    ")\n",
    "\n",
    "# 4. Generate response using the OpenAI API.\n",
    "start_time = time.time()\n",
    "response = anyscale_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {SAMPLE_QUESTION}\",}\n",
    "    ],\n",
    "    model=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    seed=RANDOM_SEED,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    "    top_p=TOP_P, \n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "llama3_anyscale_endpoints_time = time.time() - start_time\n",
    "\n",
    "# Print the response.\n",
    "pprint.pprint(response.choices[0].message.content.replace('\\n', ' '))\n",
    "print(f\"llama3_anyscale_endpoints_time: {format(llama3_anyscale_endpoints_time, '.2f')} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "309d7025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I'm happy to help!  The context is not strongly relevant, so I'll ignore it \"\n",
      " 'and provide a direct answer.  HNSW stands for Hierarchical Navigable Small '\n",
      " 'World (graph), which is a type of data structure used in information '\n",
      " 'retrieval and search algorithms. The parameters for HNSW typically include:  '\n",
      " '* m: the number of nearest neighbors to consider when searching * ef: the '\n",
      " 'maximum distance threshold for searching * r: the number of bits used to '\n",
      " 'represent each dimension  These parameters control how efficiently HNSW '\n",
      " 'searches through large datasets, balancing trade-offs between precision, '\n",
      " 'recall, and computational complexity.')\n",
      "llama3_octai_endpoints_time: 1.89 seconds\n"
     ]
    }
   ],
   "source": [
    "# Also try OctoAI\n",
    "# !python -m pip install octoai\n",
    "# import os\n",
    "from octoai.text_gen import ChatMessage\n",
    "from octoai.client import OctoAI\n",
    "\n",
    "LLM_NAME = \"meta-llama-3-8b-instruct\"\n",
    "\n",
    "octoai_client = OctoAI(\n",
    "    api_key=os.environ.get(\"OCTOAI_TOKEN\"),\n",
    ")\n",
    "\n",
    "# Generate response using OpenAI API.\n",
    "start_time = time.time()\n",
    "response = octoai_client.text_gen.create_chat_completion(\n",
    "\tmessages=[\n",
    "\t\tChatMessage(\n",
    "\t\t\tcontent=SYSTEM_PROMPT,\n",
    "\t\t\trole=\"system\"\n",
    "\t\t),\n",
    "\t\tChatMessage(\n",
    "\t\t\tcontent=SAMPLE_QUESTION,\n",
    "\t\t\trole=\"user\"\n",
    "\t\t)\n",
    "\t],\n",
    "\tmodel=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    # seed=RANDOM_SEED,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    "    top_p=TOP_P, \n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "llama3_octai_endpoints_time = time.time() - start_time\n",
    "\n",
    "# Print the response.\n",
    "pprint.pprint(response.choices[0].message.content.replace('\\n', ' '))\n",
    "print(f\"llama3_octai_endpoints_time: {format(llama3_octai_endpoints_time, '.2f')} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6b94795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The parameters for HNSW (Hierarchical Navigable Small World) indexing '\n",
      " 'algorithm are:  * M: The number of clusters in the hierarchical structure. * '\n",
      " 'efConstruction: The number of nearest neighbors to consider during the '\n",
      " 'construction of the index. * efSearch: The number of nearest neighbors to '\n",
      " 'consider during the search query.  These parameters control the trade-off '\n",
      " 'between the quality of the search results and the computational efficiency '\n",
      " 'of the algorithm.')\n",
      "llama3_groq_endpoints_time: 0.49 seconds\n"
     ]
    }
   ],
   "source": [
    "# Also try Groq endpoints\n",
    "# !python -m pip install groq\n",
    "# import os\n",
    "from groq import Groq\n",
    "\n",
    "LLM_NAME = \"llama3-8b-8192\"\n",
    "\n",
    "groq_client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Generate response using OpenAI API.\n",
    "start_time = time.time()\n",
    "response = groq_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {SAMPLE_QUESTION}\",}\n",
    "    ],\n",
    "    model=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    seed=RANDOM_SEED,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    "    top_p=TOP_P, \n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "llama3_groq_endpoints_time = time.time() - start_time\n",
    "\n",
    "# Print the response.\n",
    "pprint.pprint(response.choices[0].message.content.replace('\\n', ' '))\n",
    "print(f\"llama3_groq_endpoints_time: {format(llama3_groq_endpoints_time, '.2f')} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1facf70",
   "metadata": {},
   "source": [
    "## Also try MistralAI's Mixtral 8x7B-Instruct-v0.1\n",
    "\n",
    "This time ollama's version requires 48GB RAM. If you have big enough compute, run the command:\n",
    "> ollama run mixtral\n",
    "\n",
    "Since my laptop is a M2 with only 16GB RAM, I decided to **run Mixtral using Anyscale Endpoints**. Instructions to install. <br>\n",
    "> https://github.com/simonw/llm-anyscale-endpoints\n",
    "\n",
    "To get back to **Anyscale Endpoints** anytime, open the playground.<br>\n",
    "https://console.anyscale.com/v2/playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4581267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
      "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
      "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
      "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
      "OpenAI Chat: gpt-4-1106-preview\n",
      "OpenAI Chat: gpt-4-0125-preview\n",
      "OpenAI Chat: gpt-4-turbo-preview (aliases: gpt-4-turbo, 4-turbo, 4t)\n",
      "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
      "AnyscaleEndpoints: meta-llama/Llama-2-7b-chat-hf\n",
      "AnyscaleEndpoints: meta-llama/Llama-2-13b-chat-hf\n",
      "AnyscaleEndpoints: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "AnyscaleEndpoints: mistralai/Mistral-7B-Instruct-v0.1\n",
      "AnyscaleEndpoints: meta-llama/Llama-2-70b-chat-hf\n",
      "AnyscaleEndpoints: meta-llama/Llama-3-8b-chat-hf\n",
      "AnyscaleEndpoints: meta-llama/Llama-3-70b-chat-hf\n",
      "AnyscaleEndpoints: codellama/CodeLlama-70b-Instruct-hf\n",
      "AnyscaleEndpoints: mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "AnyscaleEndpoints: mlabonne/NeuralHermes-2.5-Mistral-7B\n",
      "AnyscaleEndpoints: google/gemma-7b-it\n"
     ]
    }
   ],
   "source": [
    "# List all the anyscale endpoint models.\n",
    "!llm models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6b16264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' The parameters for HNSW (Hierarchical Navigable Small World) in the Faiss '\n",
      " 'library refer to settings for an approximate nearest neighbor search '\n",
      " 'algorithm using a graph structure. These parameters include `M`, the number '\n",
      " 'of links per node, and `efConstruction`, the size of the candidate set '\n",
      " 'during construction.  Answer: The parameters for HNSW define settings for an '\n",
      " 'approximate nearest neighbor search algorithm using a graph structure, with '\n",
      " 'parameters such as `M`, the number of links per node, and `efConstruction`, '\n",
      " 'the size of the candidate set during construction.  Grounding sources: - '\n",
      " 'Faiss library documentation on HNSW: '\n",
      " '<https://github.com/facebookresearch/faiss/wiki/Hierarchical-Navigable-Small-World-graphs>')\n",
      "mixtral_anyscale_endpoints_time: 3.66 seconds\n"
     ]
    }
   ],
   "source": [
    "# Call Anyscale enpoint using OpenAI API.\n",
    "import openai\n",
    "\n",
    "LLM_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# 2. Get your API key: https://platform.openai.com/api-keys\n",
    "# 3. Save your api key in env variable.\n",
    "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "openai_client = openai.OpenAI(\n",
    "    base_url = \"https://api.endpoints.anyscale.com/v1\",\n",
    "    api_key=os.environ.get(\"ANYSCALE_ENPOINT_KEY\"),\n",
    ")\n",
    "\n",
    "# 4. Generate response using the OpenAI API.\n",
    "start_time = time.time()\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {SAMPLE_QUESTION}\",}\n",
    "    ],\n",
    "    model=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    seed=RANDOM_SEED,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    ")\n",
    "mixtral_anyscale_endpoints_time = time.time() - start_time\n",
    "\n",
    "# Print the response.\n",
    "pprint.pprint(response.choices[0].message.content.replace('\\n', ' '))\n",
    "print(f\"mixtral_anyscale_endpoints_time: {format(mixtral_anyscale_endpoints_time, '.2f')} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e05c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTFUL API CALL\n",
    "# import subprocess\n",
    "# command = \"llm -m mistralai/Mixtral-8x22B-Instruct-v0.1 SAMPLE_QUESTION --system SYSTEM_PROMPT\"\n",
    "\n",
    "# start_time = time.time()\n",
    "# output = subprocess.check_output(command, shell=True, text=True)\n",
    "# mixtral_anyscale_endpoints_time = time.time() - start_time\n",
    "\n",
    "# print(output)\n",
    "# print(f\"mixtral_anyscale_endpoints_time: {format(mixtral_anyscale_endpoints_time, '.2f')} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc532a8",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../images/mistral_mixtral.png\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e172726",
   "metadata": {},
   "source": [
    "## Try OpenAI to generate a human-like chat response to the user's question \n",
    "\n",
    "We've practiced retrieval for free on our own data using open-source LLMs.  <br>\n",
    "\n",
    "Now let's make a call to the paid OpenAI GPT.\n",
    "\n",
    "ðŸ’¡ Note: For use cases that need to always be factually grounded, use very low temperature values while more creative tasks can benefit from higher temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a62feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, pprint\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Define the generation llm model to use.\n",
    "# https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "# Customers using the pinned gpt-3.5-turbo model alias will be automatically upgraded to gpt-3.5-turbo-0125 two weeks after this model launches.\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "# 2. Get your API key: https://platform.openai.com/api-keys\n",
    "# 3. Save your api key in env variable.\n",
    "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "openai_client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 4. Generate response using the OpenAI API.\n",
    "start_time = time.time()\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT,},\n",
    "        {\"role\": \"user\", \"content\": f\"question: {SAMPLE_QUESTION}\",}\n",
    "    ],\n",
    "    model=LLM_NAME,\n",
    "    temperature=TEMPERATURE,\n",
    "    seed=RANDOM_SEED,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    ")\n",
    "chatgpt_35turbo_time = time.time() - start_time\n",
    "\n",
    "# Print the question and answer along with grounding sources and citations.\n",
    "print(f\"Question: {SAMPLE_QUESTION}\")\n",
    "\n",
    "# 5. Print all answers in the response.\n",
    "for i, choice in enumerate(response.choices, 1):\n",
    "    pprint.pprint(f\"Answer: {choice.message.content}\")\n",
    "    print(\"\\n\")\n",
    "print(f\"chatgpt_3.5_turbo_time: {format(chatgpt_35turbo_time, '.5f')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aeeba",
   "metadata": {},
   "source": [
    "## Use Ragas to evaluate RAG pipeline\n",
    "\n",
    "Ragas is an open source project for evaluating RAG components.  [Paper](https://arxiv.org/abs/2309.15217), [Code](https://docs.ragas.io/en/stable/getstarted/index.html), [Docs](https://docs.ragas.io/en/stable/getstarted/index.html), [Intro blog](https://medium.com/towards-data-science/rag-evaluation-using-ragas-4645a4c6c477).\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/ragas_eval_image.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "**Please note that RAGAS can use a large amount of OpenAI api token consumption.** <br> \n",
    "\n",
    "Read through this notebook carefully and pay attention to the number of questions and metrics you want to evaluate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1097990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>ground_truth_answer</th>\n",
       "      <th>Custom_RAG_context</th>\n",
       "      <th>simple_context</th>\n",
       "      <th>Sources</th>\n",
       "      <th>Custom_RAG_answer</th>\n",
       "      <th>llama3_ollama_answer</th>\n",
       "      <th>llama3_anyscale_answer</th>\n",
       "      <th>llama3_octoai_answer</th>\n",
       "      <th>llama3_groq_answer</th>\n",
       "      <th>mixtral_8x7b_anyscale_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do the parameters for HNSW mean?</td>\n",
       "      <td># M: maximum degree, or number of connections ...</td>\n",
       "      <td>In order to improve performance, HNSW limits t...</td>\n",
       "      <td>HNSW (Hierarchical Navigable Small World Graph...</td>\n",
       "      <td>https://milvus.io/docs/index.md</td>\n",
       "      <td>The parameters for HNSW (Hierarchical Navigabl...</td>\n",
       "      <td>In Hierarchical Navigable Small World (HNSW) g...</td>\n",
       "      <td>The parameters for HNSW, specifically M, efCon...</td>\n",
       "      <td>The parameters for HNSW (Hierarchical Navigabl...</td>\n",
       "      <td>The parameters for HNSW (Hierarchical Navigabl...</td>\n",
       "      <td>The parameters for HNSW include the maximum de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are good default values for HNSW paramete...</td>\n",
       "      <td>M=16, efConstruction=32, â€¨ef=32</td>\n",
       "      <td>parameters vary with Milvus distribution. Sele...</td>\n",
       "      <td>Parameter Description Range Default Value     ...</td>\n",
       "      <td>https://milvus.io/docs/index.md, https://milvu...</td>\n",
       "      <td>- `efConstruction`: 200â€¨- `M`: 16</td>\n",
       "      <td>**M** (number of factors): Since you have a re...</td>\n",
       "      <td>M=16, â€¨efConstruction=100</td>\n",
       "      <td>* M: 16 (number of factors of product quantiza...</td>\n",
       "      <td>* M: 16 (number of factors of product quantiza...</td>\n",
       "      <td>ef_construction=100, ef=200, â€¨M=16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does nlist vs nprobe mean in ivf_flat?</td>\n",
       "      <td># nlist:  controls how the vector data is part...</td>\n",
       "      <td>Search parameters  IVF_FLAT divides vector dat...</td>\n",
       "      <td>IVF_FLAT index divides a vector space into nli...</td>\n",
       "      <td>https://milvus.io/docs/index.md</td>\n",
       "      <td>\"nlist\" refers to the number of cluster units ...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td>`nlist` and `nprobe` are parameters used in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the default AUTOINDEX index and vector...</td>\n",
       "      <td>Trick answer:  Index=HNSW and vector field dis...</td>\n",
       "      <td>Index parameters dictate how Milvus organizes ...</td>\n",
       "      <td>Index parameters Index parameters dictate how ...</td>\n",
       "      <td>https://milvus.io/docs/index.md</td>\n",
       "      <td>The default AUTOINDEX index in Milvus uses the...</td>\n",
       "      <td>* `AUTOINDEX` = HNSW\\n* Default Vector Field D...</td>\n",
       "      <td>The default AUTOINDEX index type for vector fi...</td>\n",
       "      <td>The default AUTOINDEX index type for vector fi...</td>\n",
       "      <td>The default AUTOINDEX index and vector field d...</td>\n",
       "      <td>The default index for vector fields in Milvus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0              What do the parameters for HNSW mean?   \n",
       "1  What are good default values for HNSW paramete...   \n",
       "2        What does nlist vs nprobe mean in ivf_flat?   \n",
       "3  What is the default AUTOINDEX index and vector...   \n",
       "\n",
       "                                 ground_truth_answer  \\\n",
       "0  # M: maximum degree, or number of connections ...   \n",
       "1                    M=16, efConstruction=32, \n",
       "ef=32   \n",
       "2  # nlist:  controls how the vector data is part...   \n",
       "3  Trick answer:  Index=HNSW and vector field dis...   \n",
       "\n",
       "                                  Custom_RAG_context  \\\n",
       "0  In order to improve performance, HNSW limits t...   \n",
       "1  parameters vary with Milvus distribution. Sele...   \n",
       "2  Search parameters  IVF_FLAT divides vector dat...   \n",
       "3  Index parameters dictate how Milvus organizes ...   \n",
       "\n",
       "                                      simple_context  \\\n",
       "0  HNSW (Hierarchical Navigable Small World Graph...   \n",
       "1  Parameter Description Range Default Value     ...   \n",
       "2  IVF_FLAT index divides a vector space into nli...   \n",
       "3  Index parameters Index parameters dictate how ...   \n",
       "\n",
       "                                             Sources  \\\n",
       "0                    https://milvus.io/docs/index.md   \n",
       "1  https://milvus.io/docs/index.md, https://milvu...   \n",
       "2                    https://milvus.io/docs/index.md   \n",
       "3                    https://milvus.io/docs/index.md   \n",
       "\n",
       "                                   Custom_RAG_answer  \\\n",
       "0  The parameters for HNSW (Hierarchical Navigabl...   \n",
       "1                  - `efConstruction`: 200\n",
       "- `M`: 16   \n",
       "2  \"nlist\" refers to the number of cluster units ...   \n",
       "3  The default AUTOINDEX index in Milvus uses the...   \n",
       "\n",
       "                                llama3_ollama_answer  \\\n",
       "0  In Hierarchical Navigable Small World (HNSW) g...   \n",
       "1  **M** (number of factors): Since you have a re...   \n",
       "2  `nlist` and `nprobe` are two distinct paramete...   \n",
       "3  * `AUTOINDEX` = HNSW\\n* Default Vector Field D...   \n",
       "\n",
       "                              llama3_anyscale_answer  \\\n",
       "0  The parameters for HNSW, specifically M, efCon...   \n",
       "1                          M=16, \n",
       "efConstruction=100   \n",
       "2  `nlist` and `nprobe` are two distinct paramete...   \n",
       "3  The default AUTOINDEX index type for vector fi...   \n",
       "\n",
       "                                llama3_octoai_answer  \\\n",
       "0  The parameters for HNSW (Hierarchical Navigabl...   \n",
       "1  * M: 16 (number of factors of product quantiza...   \n",
       "2  `nlist` and `nprobe` are two distinct paramete...   \n",
       "3  The default AUTOINDEX index type for vector fi...   \n",
       "\n",
       "                                  llama3_groq_answer  \\\n",
       "0  The parameters for HNSW (Hierarchical Navigabl...   \n",
       "1  * M: 16 (number of factors of product quantiza...   \n",
       "2  `nlist` and `nprobe` are two distinct paramete...   \n",
       "3  The default AUTOINDEX index and vector field d...   \n",
       "\n",
       "                        mixtral_8x7b_anyscale_answer  \n",
       "0  The parameters for HNSW include the maximum de...  \n",
       "1                 ef_construction=100, ef=200, \n",
       "M=16  \n",
       "2  `nlist` and `nprobe` are parameters used in th...  \n",
       "3  The default index for vector fields in Milvus ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ragas, datasets\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Import custom functions for evaluation.\n",
    "sys.path.append(\"../evaluation\")  \n",
    "import eval_ragas as _eval_ragas\n",
    "\n",
    "# Import the evaluation metrics.\n",
    "from ragas.metrics import (\n",
    "    context_recall, \n",
    "    context_precision, \n",
    "    faithfulness, \n",
    "    answer_relevancy, \n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    "    )\n",
    "\n",
    "# Get the current working directory.\n",
    "cwd = os.getcwd()\n",
    "relative_path = '/../data/ground_truth_answers.csv'\n",
    "file_path = cwd + relative_path\n",
    "# print(f\"file_path: {file_path}\")\n",
    "\n",
    "# Read ground truth answers from file.\n",
    "eval_df = pd.read_csv(file_path, header=0, skip_blank_lines=True)\n",
    "display(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ae8d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py311-ray/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ANSWERS using 4 eval questions:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c68017dff24c4dbdcdaee0aa5ef63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate LLM: Custom_RAG_answer, avg_score: 0.6946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a56110aa4f49d0b8a5b848f5184c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate LLM: llama3_ollama_answer, avg_score: 0.6376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96fc456e3fa4c6f9fe9387b3fc4bb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate LLM: llama3_anyscale_answer, avg_score: 0.7104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4885ba4fbd734d789f2b6a5e87be4c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate LLM: llama3_octoai_answer, avg_score: 0.6894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deb74911f184fbabb42e70cdedc3b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate LLM: llama3_groq_answer, avg_score: 0.6021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdc2f8d871a4cc3b6e57f43f3e4ea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate LLM: mixtral_8x7b_anyscale_answer, avg_score: 0.752\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# Set the evaluation type.\n",
    "EVALUATE_WHAT = 'ANSWERS' \n",
    "# EVALUATE_WHAT = 'CONTEXTS'\n",
    "##########################################\n",
    "\n",
    "# Set the columns to evaluate.\n",
    "if EVALUATE_WHAT == 'CONTEXTS':\n",
    "    cols_to_evaluate=['Custom_RAG_context', 'simple_context']\n",
    "elif EVALUATE_WHAT == 'ANSWERS':\n",
    "    cols_to_evaluate=['Custom_RAG_answer', 'llama3_ollama_answer', \n",
    "                      'llama3_anyscale_answer', 'llama3_octoai_answer',\n",
    "                      'llama3_groq_answer', 'mixtral_8x7b_anyscale_answer']\n",
    "\n",
    "# Set the metrics to evaluate.\n",
    "if EVALUATE_WHAT == 'ANSWERS':\n",
    "    eval_metrics=[\n",
    "        answer_relevancy,\n",
    "        answer_similarity,\n",
    "        answer_correctness,\n",
    "        faithfulness,\n",
    "        ]\n",
    "    metrics = ['answer_relevancy', 'answer_similarity', 'answer_correctness', 'faithfulness']\n",
    "elif EVALUATE_WHAT == 'CONTEXTS':\n",
    "    eval_metrics=[\n",
    "        context_recall, \n",
    "        context_precision,\n",
    "        ]\n",
    "    metrics = ['context_recall', 'context_precision']\n",
    "    \n",
    "# Change the default the llm-as-critic model.\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "ragas_llm = ragas.llms.llm_factory(model=LLM_NAME)\n",
    "\n",
    "# Change the default embeddings models to HuggingFace models.\n",
    "EMB_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "lc_embeddings = HuggingFaceEmbeddings(model_name=EMB_NAME)\n",
    "ragas_emb = LangchainEmbeddingsWrapper(embeddings=lc_embeddings)\n",
    "\n",
    "# Change embeddings and critic models for each metric.\n",
    "for metric in metrics:\n",
    "    globals()[metric].llm = ragas_llm\n",
    "    globals()[metric].embeddings = ragas_emb\n",
    "\n",
    "# Execute the evaluation.\n",
    "print(f\"Evaluating {EVALUATE_WHAT} using {eval_df.shape[0]} eval questions:\")\n",
    "ragas_result, scores = _eval_ragas.evaluate_ragas_model(\n",
    "    eval_df, \n",
    "    eval_metrics, \n",
    "    what_to_evaluate=EVALUATE_WHAT,\n",
    "    cols_to_evaluate=cols_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bad9cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mixtral_8x7b_anyscale_answer': 0.752},\n",
      " {'llama3_anyscale_answer': 0.7104},\n",
      " {'Custom_RAG_answer': 0.6946},\n",
      " {'llama3_octoai_answer': 0.6894},\n",
      " {'llama3_ollama_answer': 0.6376},\n",
      " {'llama3_groq_answer': 0.6021}]\n",
      "mixtral_8x7b_anyscale_answer 25.0% improvement over llama3_groq_answer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>avg_answer_score</th>\n",
       "      <th>evaluated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do the parameters for HNSW mean?</td>\n",
       "      <td>[In order to improve performance, HNSW limits ...</td>\n",
       "      <td>The parameters for HNSW (Hierarchical Navigabl...</td>\n",
       "      <td># M: maximum degree, or number of connections ...</td>\n",
       "      <td>0.741975</td>\n",
       "      <td>0.751567</td>\n",
       "      <td>0.596983</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.696841</td>\n",
       "      <td>Custom_RAG_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are good default values for HNSW paramete...</td>\n",
       "      <td>[parameters vary with Milvus distribution. Sel...</td>\n",
       "      <td>- `efConstruction`: 200â€¨- `M`: 16</td>\n",
       "      <td>M=16, efConstruction=32, â€¨ef=32</td>\n",
       "      <td>0.606798</td>\n",
       "      <td>0.872018</td>\n",
       "      <td>0.518005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.665607</td>\n",
       "      <td>Custom_RAG_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does nlist vs nprobe mean in ivf_flat?</td>\n",
       "      <td>[Search parameters  IVF_FLAT divides vector da...</td>\n",
       "      <td>\"nlist\" refers to the number of cluster units ...</td>\n",
       "      <td># nlist:  controls how the vector data is part...</td>\n",
       "      <td>0.724203</td>\n",
       "      <td>0.881062</td>\n",
       "      <td>0.720265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775177</td>\n",
       "      <td>Custom_RAG_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the default AUTOINDEX index and vector...</td>\n",
       "      <td>[Index parameters dictate how Milvus organizes...</td>\n",
       "      <td>The default AUTOINDEX index in Milvus uses the...</td>\n",
       "      <td>Trick answer:  Index=HNSW and vector field dis...</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.755001</td>\n",
       "      <td>0.188750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640892</td>\n",
       "      <td>Custom_RAG_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do the parameters for HNSW mean?</td>\n",
       "      <td>[In order to improve performance, HNSW limits ...</td>\n",
       "      <td>In Hierarchical Navigable Small World (HNSW) g...</td>\n",
       "      <td># M: maximum degree, or number of connections ...</td>\n",
       "      <td>0.669790</td>\n",
       "      <td>0.775061</td>\n",
       "      <td>0.527099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.657317</td>\n",
       "      <td>llama3_ollama_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are good default values for HNSW paramete...</td>\n",
       "      <td>[parameters vary with Milvus distribution. Sel...</td>\n",
       "      <td>**M** (number of factors): Since you have a re...</td>\n",
       "      <td>M=16, efConstruction=32, â€¨ef=32</td>\n",
       "      <td>0.643281</td>\n",
       "      <td>0.735836</td>\n",
       "      <td>0.183959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521025</td>\n",
       "      <td>llama3_ollama_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does nlist vs nprobe mean in ivf_flat?</td>\n",
       "      <td>[Search parameters  IVF_FLAT divides vector da...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td># nlist:  controls how the vector data is part...</td>\n",
       "      <td>0.802967</td>\n",
       "      <td>0.874736</td>\n",
       "      <td>0.468684</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.715462</td>\n",
       "      <td>llama3_ollama_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the default AUTOINDEX index and vector...</td>\n",
       "      <td>[Index parameters dictate how Milvus organizes...</td>\n",
       "      <td>* `AUTOINDEX` = HNSW\\n* Default Vector Field D...</td>\n",
       "      <td>Trick answer:  Index=HNSW and vector field dis...</td>\n",
       "      <td>0.900790</td>\n",
       "      <td>0.855149</td>\n",
       "      <td>0.213787</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.656576</td>\n",
       "      <td>llama3_ollama_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What do the parameters for HNSW mean?</td>\n",
       "      <td>[In order to improve performance, HNSW limits ...</td>\n",
       "      <td>The parameters for HNSW, specifically M, efCon...</td>\n",
       "      <td># M: maximum degree, or number of connections ...</td>\n",
       "      <td>0.798498</td>\n",
       "      <td>0.822214</td>\n",
       "      <td>0.634125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.751612</td>\n",
       "      <td>llama3_anyscale_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are good default values for HNSW paramete...</td>\n",
       "      <td>[parameters vary with Milvus distribution. Sel...</td>\n",
       "      <td>M=16, â€¨efConstruction=100</td>\n",
       "      <td>M=16, efConstruction=32, â€¨ef=32</td>\n",
       "      <td>0.598712</td>\n",
       "      <td>0.914419</td>\n",
       "      <td>0.528605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680578</td>\n",
       "      <td>llama3_anyscale_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What does nlist vs nprobe mean in ivf_flat?</td>\n",
       "      <td>[Search parameters  IVF_FLAT divides vector da...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td># nlist:  controls how the vector data is part...</td>\n",
       "      <td>0.834474</td>\n",
       "      <td>0.878586</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769235</td>\n",
       "      <td>llama3_anyscale_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the default AUTOINDEX index and vector...</td>\n",
       "      <td>[Index parameters dictate how Milvus organizes...</td>\n",
       "      <td>The default AUTOINDEX index type for vector fi...</td>\n",
       "      <td>Trick answer:  Index=HNSW and vector field dis...</td>\n",
       "      <td>0.941719</td>\n",
       "      <td>0.783050</td>\n",
       "      <td>0.195763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.640178</td>\n",
       "      <td>llama3_anyscale_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What do the parameters for HNSW mean?</td>\n",
       "      <td>[In order to improve performance, HNSW limits ...</td>\n",
       "      <td>The parameters for HNSW (Hierarchical Navigabl...</td>\n",
       "      <td># M: maximum degree, or number of connections ...</td>\n",
       "      <td>0.728416</td>\n",
       "      <td>0.762739</td>\n",
       "      <td>0.640685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.710613</td>\n",
       "      <td>llama3_octoai_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are good default values for HNSW paramete...</td>\n",
       "      <td>[parameters vary with Milvus distribution. Sel...</td>\n",
       "      <td>* M: 16 (number of factors of product quantiza...</td>\n",
       "      <td>M=16, efConstruction=32, â€¨ef=32</td>\n",
       "      <td>0.591757</td>\n",
       "      <td>0.794186</td>\n",
       "      <td>0.627118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671020</td>\n",
       "      <td>llama3_octoai_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What does nlist vs nprobe mean in ivf_flat?</td>\n",
       "      <td>[Search parameters  IVF_FLAT divides vector da...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td># nlist:  controls how the vector data is part...</td>\n",
       "      <td>0.834474</td>\n",
       "      <td>0.878586</td>\n",
       "      <td>0.492374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735144</td>\n",
       "      <td>llama3_octoai_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the default AUTOINDEX index and vector...</td>\n",
       "      <td>[Index parameters dictate how Milvus organizes...</td>\n",
       "      <td>The default AUTOINDEX index type for vector fi...</td>\n",
       "      <td>Trick answer:  Index=HNSW and vector field dis...</td>\n",
       "      <td>0.943990</td>\n",
       "      <td>0.783050</td>\n",
       "      <td>0.195763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.640934</td>\n",
       "      <td>llama3_octoai_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What do the parameters for HNSW mean?</td>\n",
       "      <td>[In order to improve performance, HNSW limits ...</td>\n",
       "      <td>The parameters for HNSW (Hierarchical Navigabl...</td>\n",
       "      <td># M: maximum degree, or number of connections ...</td>\n",
       "      <td>0.773102</td>\n",
       "      <td>0.773006</td>\n",
       "      <td>0.568252</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704787</td>\n",
       "      <td>llama3_groq_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are good default values for HNSW paramete...</td>\n",
       "      <td>[parameters vary with Milvus distribution. Sel...</td>\n",
       "      <td>* M: 16 (number of factors of product quantiza...</td>\n",
       "      <td>M=16, efConstruction=32, â€¨ef=32</td>\n",
       "      <td>0.635317</td>\n",
       "      <td>0.640155</td>\n",
       "      <td>0.722539</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666004</td>\n",
       "      <td>llama3_groq_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What does nlist vs nprobe mean in ivf_flat?</td>\n",
       "      <td>[Search parameters  IVF_FLAT divides vector da...</td>\n",
       "      <td>`nlist` and `nprobe` are two distinct paramete...</td>\n",
       "      <td># nlist:  controls how the vector data is part...</td>\n",
       "      <td>0.832889</td>\n",
       "      <td>0.878586</td>\n",
       "      <td>0.433932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715135</td>\n",
       "      <td>llama3_groq_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the default AUTOINDEX index and vector...</td>\n",
       "      <td>[Index parameters dictate how Milvus organizes...</td>\n",
       "      <td>The default AUTOINDEX index and vector field d...</td>\n",
       "      <td>Trick answer:  Index=HNSW and vector field dis...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.774264</td>\n",
       "      <td>0.193566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.322610</td>\n",
       "      <td>llama3_groq_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What do the parameters for HNSW mean?</td>\n",
       "      <td>[In order to improve performance, HNSW limits ...</td>\n",
       "      <td>The parameters for HNSW include the maximum de...</td>\n",
       "      <td># M: maximum degree, or number of connections ...</td>\n",
       "      <td>0.919632</td>\n",
       "      <td>0.810253</td>\n",
       "      <td>0.664102</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.797995</td>\n",
       "      <td>mixtral_8x7b_anyscale_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What are good default values for HNSW paramete...</td>\n",
       "      <td>[parameters vary with Milvus distribution. Sel...</td>\n",
       "      <td>ef_construction=100, ef=200, â€¨M=16</td>\n",
       "      <td>M=16, efConstruction=32, â€¨ef=32</td>\n",
       "      <td>0.610103</td>\n",
       "      <td>0.854491</td>\n",
       "      <td>0.463623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642739</td>\n",
       "      <td>mixtral_8x7b_anyscale_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What does nlist vs nprobe mean in ivf_flat?</td>\n",
       "      <td>[Search parameters  IVF_FLAT divides vector da...</td>\n",
       "      <td>`nlist` and `nprobe` are parameters used in th...</td>\n",
       "      <td># nlist:  controls how the vector data is part...</td>\n",
       "      <td>0.949621</td>\n",
       "      <td>0.851410</td>\n",
       "      <td>0.587852</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.796295</td>\n",
       "      <td>mixtral_8x7b_anyscale_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the default AUTOINDEX index and vector...</td>\n",
       "      <td>[Index parameters dictate how Milvus organizes...</td>\n",
       "      <td>The default index for vector fields in Milvus ...</td>\n",
       "      <td>Trick answer:  Index=HNSW and vector field dis...</td>\n",
       "      <td>0.912119</td>\n",
       "      <td>0.720256</td>\n",
       "      <td>0.680064</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.770813</td>\n",
       "      <td>mixtral_8x7b_anyscale_answer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0               What do the parameters for HNSW mean?   \n",
       "1   What are good default values for HNSW paramete...   \n",
       "2         What does nlist vs nprobe mean in ivf_flat?   \n",
       "3   What is the default AUTOINDEX index and vector...   \n",
       "4               What do the parameters for HNSW mean?   \n",
       "5   What are good default values for HNSW paramete...   \n",
       "6         What does nlist vs nprobe mean in ivf_flat?   \n",
       "7   What is the default AUTOINDEX index and vector...   \n",
       "8               What do the parameters for HNSW mean?   \n",
       "9   What are good default values for HNSW paramete...   \n",
       "10        What does nlist vs nprobe mean in ivf_flat?   \n",
       "11  What is the default AUTOINDEX index and vector...   \n",
       "12              What do the parameters for HNSW mean?   \n",
       "13  What are good default values for HNSW paramete...   \n",
       "14        What does nlist vs nprobe mean in ivf_flat?   \n",
       "15  What is the default AUTOINDEX index and vector...   \n",
       "16              What do the parameters for HNSW mean?   \n",
       "17  What are good default values for HNSW paramete...   \n",
       "18        What does nlist vs nprobe mean in ivf_flat?   \n",
       "19  What is the default AUTOINDEX index and vector...   \n",
       "20              What do the parameters for HNSW mean?   \n",
       "21  What are good default values for HNSW paramete...   \n",
       "22        What does nlist vs nprobe mean in ivf_flat?   \n",
       "23  What is the default AUTOINDEX index and vector...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [In order to improve performance, HNSW limits ...   \n",
       "1   [parameters vary with Milvus distribution. Sel...   \n",
       "2   [Search parameters  IVF_FLAT divides vector da...   \n",
       "3   [Index parameters dictate how Milvus organizes...   \n",
       "4   [In order to improve performance, HNSW limits ...   \n",
       "5   [parameters vary with Milvus distribution. Sel...   \n",
       "6   [Search parameters  IVF_FLAT divides vector da...   \n",
       "7   [Index parameters dictate how Milvus organizes...   \n",
       "8   [In order to improve performance, HNSW limits ...   \n",
       "9   [parameters vary with Milvus distribution. Sel...   \n",
       "10  [Search parameters  IVF_FLAT divides vector da...   \n",
       "11  [Index parameters dictate how Milvus organizes...   \n",
       "12  [In order to improve performance, HNSW limits ...   \n",
       "13  [parameters vary with Milvus distribution. Sel...   \n",
       "14  [Search parameters  IVF_FLAT divides vector da...   \n",
       "15  [Index parameters dictate how Milvus organizes...   \n",
       "16  [In order to improve performance, HNSW limits ...   \n",
       "17  [parameters vary with Milvus distribution. Sel...   \n",
       "18  [Search parameters  IVF_FLAT divides vector da...   \n",
       "19  [Index parameters dictate how Milvus organizes...   \n",
       "20  [In order to improve performance, HNSW limits ...   \n",
       "21  [parameters vary with Milvus distribution. Sel...   \n",
       "22  [Search parameters  IVF_FLAT divides vector da...   \n",
       "23  [Index parameters dictate how Milvus organizes...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   The parameters for HNSW (Hierarchical Navigabl...   \n",
       "1                   - `efConstruction`: 200\n",
       "- `M`: 16   \n",
       "2   \"nlist\" refers to the number of cluster units ...   \n",
       "3   The default AUTOINDEX index in Milvus uses the...   \n",
       "4   In Hierarchical Navigable Small World (HNSW) g...   \n",
       "5   **M** (number of factors): Since you have a re...   \n",
       "6   `nlist` and `nprobe` are two distinct paramete...   \n",
       "7   * `AUTOINDEX` = HNSW\\n* Default Vector Field D...   \n",
       "8   The parameters for HNSW, specifically M, efCon...   \n",
       "9                           M=16, \n",
       "efConstruction=100   \n",
       "10  `nlist` and `nprobe` are two distinct paramete...   \n",
       "11  The default AUTOINDEX index type for vector fi...   \n",
       "12  The parameters for HNSW (Hierarchical Navigabl...   \n",
       "13  * M: 16 (number of factors of product quantiza...   \n",
       "14  `nlist` and `nprobe` are two distinct paramete...   \n",
       "15  The default AUTOINDEX index type for vector fi...   \n",
       "16  The parameters for HNSW (Hierarchical Navigabl...   \n",
       "17  * M: 16 (number of factors of product quantiza...   \n",
       "18  `nlist` and `nprobe` are two distinct paramete...   \n",
       "19  The default AUTOINDEX index and vector field d...   \n",
       "20  The parameters for HNSW include the maximum de...   \n",
       "21                 ef_construction=100, ef=200, \n",
       "M=16   \n",
       "22  `nlist` and `nprobe` are parameters used in th...   \n",
       "23  The default index for vector fields in Milvus ...   \n",
       "\n",
       "                                         ground_truth  answer_relevancy  \\\n",
       "0   # M: maximum degree, or number of connections ...          0.741975   \n",
       "1                     M=16, efConstruction=32, \n",
       "ef=32          0.606798   \n",
       "2   # nlist:  controls how the vector data is part...          0.724203   \n",
       "3   Trick answer:  Index=HNSW and vector field dis...          0.978927   \n",
       "4   # M: maximum degree, or number of connections ...          0.669790   \n",
       "5                     M=16, efConstruction=32, \n",
       "ef=32          0.643281   \n",
       "6   # nlist:  controls how the vector data is part...          0.802967   \n",
       "7   Trick answer:  Index=HNSW and vector field dis...          0.900790   \n",
       "8   # M: maximum degree, or number of connections ...          0.798498   \n",
       "9                     M=16, efConstruction=32, \n",
       "ef=32          0.598712   \n",
       "10  # nlist:  controls how the vector data is part...          0.834474   \n",
       "11  Trick answer:  Index=HNSW and vector field dis...          0.941719   \n",
       "12  # M: maximum degree, or number of connections ...          0.728416   \n",
       "13                    M=16, efConstruction=32, \n",
       "ef=32          0.591757   \n",
       "14  # nlist:  controls how the vector data is part...          0.834474   \n",
       "15  Trick answer:  Index=HNSW and vector field dis...          0.943990   \n",
       "16  # M: maximum degree, or number of connections ...          0.773102   \n",
       "17                    M=16, efConstruction=32, \n",
       "ef=32          0.635317   \n",
       "18  # nlist:  controls how the vector data is part...          0.832889   \n",
       "19  Trick answer:  Index=HNSW and vector field dis...          0.000000   \n",
       "20  # M: maximum degree, or number of connections ...          0.919632   \n",
       "21                    M=16, efConstruction=32, \n",
       "ef=32          0.610103   \n",
       "22  # nlist:  controls how the vector data is part...          0.949621   \n",
       "23  Trick answer:  Index=HNSW and vector field dis...          0.912119   \n",
       "\n",
       "    answer_similarity  answer_correctness  faithfulness  avg_answer_score  \\\n",
       "0            0.751567            0.596983      0.500000          0.696841   \n",
       "1            0.872018            0.518005      0.000000          0.665607   \n",
       "2            0.881062            0.720265      1.000000          0.775177   \n",
       "3            0.755001            0.188750      0.000000          0.640892   \n",
       "4            0.775061            0.527099      1.000000          0.657317   \n",
       "5            0.735836            0.183959      0.000000          0.521025   \n",
       "6            0.874736            0.468684      0.285714          0.715462   \n",
       "7            0.855149            0.213787      0.500000          0.656576   \n",
       "8            0.822214            0.634125      1.000000          0.751612   \n",
       "9            0.914419            0.528605      0.000000          0.680578   \n",
       "10           0.878586            0.594646      1.000000          0.769235   \n",
       "11           0.783050            0.195763      1.000000          0.640178   \n",
       "12           0.762739            0.640685      1.000000          0.710613   \n",
       "13           0.794186            0.627118      0.000000          0.671020   \n",
       "14           0.878586            0.492374      1.000000          0.735144   \n",
       "15           0.783050            0.195763      1.000000          0.640934   \n",
       "16           0.773006            0.568252      1.000000          0.704787   \n",
       "17           0.640155            0.722539      0.333333          0.666004   \n",
       "18           0.878586            0.433932      1.000000          0.715135   \n",
       "19           0.774264            0.193566           NaN          0.322610   \n",
       "20           0.810253            0.664102      0.333333          0.797995   \n",
       "21           0.854491            0.463623      0.000000          0.642739   \n",
       "22           0.851410            0.587852      0.500000          0.796295   \n",
       "23           0.720256            0.680064      0.333333          0.770813   \n",
       "\n",
       "                       evaluated  \n",
       "0              Custom_RAG_answer  \n",
       "1              Custom_RAG_answer  \n",
       "2              Custom_RAG_answer  \n",
       "3              Custom_RAG_answer  \n",
       "4           llama3_ollama_answer  \n",
       "5           llama3_ollama_answer  \n",
       "6           llama3_ollama_answer  \n",
       "7           llama3_ollama_answer  \n",
       "8         llama3_anyscale_answer  \n",
       "9         llama3_anyscale_answer  \n",
       "10        llama3_anyscale_answer  \n",
       "11        llama3_anyscale_answer  \n",
       "12          llama3_octoai_answer  \n",
       "13          llama3_octoai_answer  \n",
       "14          llama3_octoai_answer  \n",
       "15          llama3_octoai_answer  \n",
       "16            llama3_groq_answer  \n",
       "17            llama3_groq_answer  \n",
       "18            llama3_groq_answer  \n",
       "19            llama3_groq_answer  \n",
       "20  mixtral_8x7b_anyscale_answer  \n",
       "21  mixtral_8x7b_anyscale_answer  \n",
       "22  mixtral_8x7b_anyscale_answer  \n",
       "23  mixtral_8x7b_anyscale_answer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate and print the percent improvements.\n",
    "if EVALUATE_WHAT == 'ANSWERS':\n",
    "    # Sort scores from highest to lowest\n",
    "    sorted_scores = sorted(scores, key=lambda item: sum(item.values()), reverse=True)\n",
    "    pprint.pprint(sorted_scores)\n",
    "    # Calculate the percent improvement of the best LLM over the worst LLM.\n",
    "    highest_score = list(sorted_scores[0].values())[0]\n",
    "    lowest_score = list(sorted_scores[-1].values())[0]\n",
    "    best_llm = list(sorted_scores[0].keys())[0]\n",
    "    worst_llm = list(sorted_scores[-1].keys())[0]\n",
    "    percent_better = (highest_score - lowest_score) / lowest_score * 100\n",
    "    print(f\"{best_llm} {np.round(percent_better,0)}% improvement over {worst_llm}.\")\n",
    "\n",
    "elif EVALUATE_WHAT == 'CONTEXTS':\n",
    "    pprint.pprint(scores)\n",
    "    percent_better = (scores[0]['Custom_RAG_context'] - scores[1]['simple_context']) \\\n",
    "                     / scores[1]['simple_context'] * 100\n",
    "    print(f\"HTML chunking {np.round(percent_better,0)}% improvement over Simple chunking.\")\n",
    "\n",
    "# Display the evaluation details.\n",
    "display(ragas_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c408624",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Avg Context Precision htmlsplitter score = 0.42 (147% improvement)\n",
    "# Avg Context Precision simple score = 0.17\n",
    "####################################################\n",
    "\n",
    "####################################################\n",
    "# Avg mistralai mixtral_8x7b_instruct score = 0.7382 (21% improvement over llama3_groq_answer)\n",
    "# Avg llama_3_70b_octoai_instruct score = 0.7147\n",
    "# Avg llama3_70b_anyscale_chat score = 0.7102\n",
    "# Avg openai gpt-3.5-turbo score = 0.699 \n",
    "# Avg llama_3_8b_ollama_instruct score = 0.6354 \n",
    "# Avg anthropic_claud3_haiku_answer score = 0.6172\n",
    "# Avg llama3_70b_groq_instruct score = 0.6121\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0e81e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop collection\n",
    "utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c777937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Christy Bergman\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "torch    : 2.2.2\n",
      "datasets : 2.19.0\n",
      "pymilvus : 2.4.2\n",
      "langchain: 0.1.16\n",
      "ollama   : 0.1.8\n",
      "anthropic: 0.25.6\n",
      "openai   : 1.30.1\n",
      "ragas    : 0.1.7\n",
      "\n",
      "conda environment: py311-ray\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Props to Sebastian Raschka for this handy watermark.\n",
    "# !pip install watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a 'Christy Bergman' -v -p torch,datasets,pymilvus,langchain,ollama,anthropic,openai,ragas --conda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
